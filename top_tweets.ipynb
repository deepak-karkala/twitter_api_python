{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's trending on Twitter now ?\n",
    "\n",
    "This project aims to find out the trending topics on Twitter in real time based on the frequencies of the hashtags used. Twitter API is used to get the streaming data and the results are then processed to determine the trending topics. Python is used to process the data. The library 'tweepy' is used to get the streaming data from API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tweepy\n",
    "import json\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to access Twitter Streaming data, a Twitter account needs to be created and the following four keys needs to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The streaming data can be extracted by extending the 'StreamListener' Class. By default, Twitter's Streaming data will be continuous and will not stop. In order to restrict the number of tweets collected, a time limit is set in the constructor of the extended class. As long as the time limit is not exceeded, the 'on_data' method will write the Tweets collected in real time to a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyListener(StreamListener):\n",
    "    def __init__(self, time_limit=60):\n",
    "        self.start_time = time.time()\n",
    "        self.limit = time_limit\n",
    "        self.saveFile = open('python.json', 'a')\n",
    "        super(MyListener, self).__init__()\n",
    "            \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            if (time.time() - self.start_time) < self.limit:\n",
    "                with open('python.json', 'a') as f:\n",
    "                    f.write(data)\n",
    "                    return True\n",
    "            else:\n",
    "                self.saveFile.close()\n",
    "                return False\n",
    "\n",
    "        except BaseException as e:\n",
    "            print((\"Error on_data: %s\") % str(e))\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweepy provides methods to get access to streaming data. The keys generated will be used to gain access to the data. As mentioned above, Twitter's streaming data will enter an infinite loop by default and may block the other processes. In order to avoid this, the 'async' parameter in set to True in the 'Stream' object. Further by using the parameter 'track' only those tweets containing the desired terms can be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n",
      "406\n"
     ]
    }
   ],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "#twitter_stream.filter(async=True)\n",
    "twitter_stream.filter(track=['a'], async=True)\n",
    "\n",
    "## Code to print the tweets on one's timeline.   \n",
    "#api = tweepy.API(auth)\n",
    "#public_tweets = api.home_timeline()\n",
    "#for tweet in public_tweets:\n",
    "#    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets collected were saved in a file in JSON format. The following piece of code will then extract the tweets from the file and save it as a Python List."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "with open('python.json', 'r') as f:\n",
    "    for line in f:\n",
    "        tweets.append(json.loads(line))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collected tweets will have a number of parameters besides the actual text of the tweets. In order to extract the hashtags, the text in each of the tweets is split into words and each word is then scanned to see if it begins with the character '#'. The extracted hashtags can then be stored in a Python List."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtag = []\n",
    "for i in range(len(tweets)):\n",
    "    \n",
    "    ## It looks like attribute 'text' is missing in some tweets\n",
    "    ## Not using such tweets\n",
    "    flag=0\n",
    "    for key, value in tweets[i].items():\n",
    "        if key == 'text':\n",
    "            flag=1\n",
    "            break\n",
    "        else:\n",
    "            flag=0\n",
    "    \n",
    "    if flag==1:\n",
    "        for text in tweets[i]['text'].split(\" \"):\n",
    "            if text.startswith('#'):\n",
    "                hashtag.append(text[1:])\n",
    "#hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Python dictionary is then created to store the frequencies of occurrences of each hashtag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtag_dict = {}\n",
    "for i in hashtag:\n",
    "    if i in hashtag_dict:\n",
    "        hashtag_dict[i] += 1\n",
    "    else:\n",
    "        hashtag_dict[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 'N' hashtags can then be computed. This can be used as a measure to study the trending topics on Twitter based on the tweets collected over a period of 'T' seconds in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TreCru', 35),\n",
       " ('인피니트', 9),\n",
       " ('태풍', 9),\n",
       " ('DialogoEsHambruna', 5),\n",
       " ('Win', 5),\n",
       " ('4DaysToLion…', 5),\n",
       " ('FelizLunes', 5),\n",
       " ('WIN', 5),\n",
       " ('Comp', 3),\n",
       " ('NationalBoyfriendDay', 3)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "hashtags_sorted = sorted(hashtag_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "hashtags_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of this writing, the game 'Treasure Cruise' seem to be trending with a total of 35 tweets with the hashtags 'TreCru' over a period of 60s."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
